# gRPC API Documentation

This document describes gRPC contracts, request metadata (including `correlation_id`), standardized telemetry events and their semantics, error model, as well as production and security practices.

**Doc version**: Updated 2025-11-09 — added sections on retries, error handling, deduplication, and correlation with system events. When changing API/events/metadata, update the documentation. In CI, you can substitute the actual SHA via `GITHUB_SHA` to link the document to a specific build.

## Table of Contents

- [Service Overview](#service-overview)
- [Request Metadata](#request-metadata)
  - [Correlation ID](#correlation-id)
  - [Other Metadata](#other-metadata)
- [Telemetry and Observability](#telemetry-and-observability)
  - [Events](#events)
  - [Metadata (unified format)](#metadata-unified-format)
  - [Measurements](#measurements)
  - [Semantics of `count`](#semantics-of-count)
- [Error Handling](#error-handling)
  - [Unified Model](#unified-model)
  - [Error Structure](#error-structure)
  - [Canonical reasons (summary)](#canonical-reasons-summary)
  - [List of Error Atoms](#list-of-error-atoms)
  - [Retryability (UNAVAILABLE vs RESOURCE_EXHAUSTED)](#retryability-unavailable-vs-resource_exhausted)
- [Performance](#performance)
  - [Operation Complexity](#operation-complexity)
  - [Threshold Values](#threshold-values)
  - [Monitoring Recommendations](#monitoring-recommendations)
  - [Dashboards and Alerts](#dashboards-and-alerts)
  - [Playbook: Diagnostics and Correlation](#playbook-diagnostics-and-correlation)
  - [Playbook: Correlation with System Events](#playbook-correlation-with-system-events)
- [ETS Transfer Protocol](#ets-transfer-protocol)
  - [Recovery Stages](#recovery-stages)
  - [Transfer Events](#transfer-events)
  - [Requirements](#requirements)
- [Security](#security)
  - [Prohibition of Secret Logging](#prohibition-of-secret-logging)
  - [Authorization](#authorization)
- [Usage Examples](#usage-examples)
  - [Calling ListPolicies with Metadata](#calling-listpolicies-with-metadata)
  - [Example transfer_success Event](#example-transfer_success-event)
  - [Example transferred_to_heir Event](#example-transferred_to_heir-event)
- [Conventions](#conventions)
  - [Checklist for New Events](#checklist-for-new-events)
  - [Implementation of `otp_version` and `service` in Telemetry](#implementation-of-otp_version-and-service-in-telemetry)
  - [Document Version](#document-version)

## Service Overview

- **Router.Decide** (`router_grpc`):
  - `Decide`: making routing decisions for messages
  - Public API (authorization not required)
  - **Proto**: [`apps/otp/router/proto/beamline/flow/v1/flow.proto`](../proto/beamline/flow/v1/flow.proto) — service `Router`, RPC `Decide`

- **RouterAdmin** (`router_admin_grpc`):
  - `UpsertPolicy`: create/update policy for `tenant_id`
  - `DeletePolicy`: delete policy by `policy_id`
  - `GetPolicy`: get a single policy by `policy_id`
  - `ListPolicies`: get list of policies for `tenant_id`
  - Requires authorization via API key in metadata
  - **Proto**: [`apps/otp/router/proto/beamline/flow/v1/flow.proto`](../proto/beamline/flow/v1/flow.proto) — service `RouterAdmin`, RPC `UpsertPolicy`/`DeletePolicy`/`ListPolicies`/`GetPolicy`

- **Policy Store** (internal operations `router_policy_store`):
  - `upsert_policy`, `delete_policy`, `list_policies`, `get_policy`, `rebuild_index`
  - ETS transfer: `claim → wait_for_table_owner_with_retry → fallback create`
  - **Proto**: Does not use gRPC (internal module), but policy semantics are defined in [`apps/otp/router/proto/beamline/flow/v1/flow.proto`](../proto/beamline/flow/v1/flow.proto) — messages `Policy`, `Provider`

**Note**: Exact RPC and message names are defined in [`apps/otp/router/proto/beamline/flow/v1/flow.proto`](../proto/beamline/flow/v1/flow.proto). Semantics and contracts are documented here.

**Stability of `.proto` links**: Links to `.proto` files use relative paths for stability. In CI, you can generate exact links to a specific commit via environment variable (e.g., `GITHUB_SHA`) to link to a specific build.

## Request Metadata

### Correlation ID

- **Format**: UUID v4 or ULID (string, generated by client)
  - **UUID v4**: 36 characters (e.g., `550e8400-e29b-41d4-a716-446655440000`), alphabet: `[0-9a-fA-F-]`
  - **ULID**: 26 characters (e.g., `01ARZ3NDEKTSV4RRFFQ69G5FAV`), alphabet: Crockford's Base32 without `I`, `L`, `O`, `U` (i.e., `0123456789ABCDEFGHJKMNPQRSTVWXYZ`)
- **Source**: gRPC metadata via header **`x-correlation-id`** (canonical) or `correlation-id` (fallback for compatibility)
- **gRPC metadata rules**:
  - All metadata keys must be lowercase ASCII (e.g., `x-correlation-id`, not `X-Correlation-Id`)
  - Binary keys must have `-bin` suffix (e.g., `grpc-status-details-bin`)
  - For `x-correlation-id`, `-bin` suffix is not needed, as the value is text (UUID/ULID string)
- **Binary metadata (`-bin`)**:
  - For keys with `-bin` suffix, values are transmitted as binary bytes; many client implementations encode values in base64 for HTTP/2 transport layer
  - Example documentation assumes text values (not `-bin`) for `x-correlation-id`
- **Case rule**: HTTP/2/gRPC metadata are normalized to lowercase; use **`x-correlation-id`** in lowercase in all examples and client requests for consistency
  - **Important**: Use lowercase `x-correlation-id` for clients and servers; avoid mixed-case keys (e.g., `X-Correlation-Id` or `X-CORRELATION-ID`) to eliminate ambiguity for integrators
- **Key compatibility**:
  - Only lowercase keys `x-correlation-id` and `correlation-id` are supported
  - Variants with different case (e.g., `X-Correlation-Id`, `Correlation-ID`) are considered invalid and ignored
  - This removes false expectations and ensures consistent metadata processing
- **Extraction**: `extract_correlation_id/1` in [`router_grpc.erl`](../src/router_grpc.erl) and [`router_admin_grpc.erl`](../src/router_admin_grpc.erl)
  - **Behavior**: Function first searches for canonical key `x-correlation-id` (lowercase) in metadata; if not found, uses fallback `correlation-id` (lowercase). This ensures consistency between clients and server, eliminating discrepancies.
  - **Property-based testing**: It is recommended to use property-based tests for `extract_correlation_id/1`:
    - Trimming check: leading and trailing spaces are trimmed
    - Lowercase key check: `x-correlation-id` and `correlation-id` (lowercase) are extracted correctly
    - Fallback priority check: `x-correlation-id` has priority over `correlation-id`
    - UUID v4 and ULID validation: correct formats are accepted, incorrect — rejected
    - `invalid_correlation_id` publication: invalid values are logged with `reason` (without blocking request)
    - **Additional cases**:
      - Missing keys: when both keys are absent, `undefined` is returned
      - Key conflict: when both keys (`x-correlation-id` and `correlation-id`) are present with different values, `x-correlation-id` has priority
      - Binary keys are ignored: keys with `-bin` suffix (e.g., `x-correlation-id-bin`) are ignored for `correlation_id`
- **`correlation_id` value hygiene**:
  - Space trimming: value must be trimmed of leading and trailing spaces before use
  - Empty string prohibition: empty string after trimming is treated as absence of value (`undefined`)
  - Format validation: value must match UUID v4 or ULID format (Crockford's Base32)
  - Invalid value logging: on format mismatch, logged as `invalid_correlation_id` (does not block request, but recorded in logs for analysis)
  - **Noise reduction**: It is recommended to limit publication frequency (`rate_limit`) and/or apply sampling under high load to avoid noise in alerts
  - **Diagnostics**: Mismatch reasons can be reflected in `reason` field (e.g., `bad_uuid`, `bad_ulid`, `empty`, `whitespace`) to improve diagnostics
- **ULID monotonicity**: ULID provides monotonicity only within process/hours; with multi-node generations, order "jumps" are possible
  - Rely on explicit `operation_id`/`transfer_id` for correlation, not ULID chronology
  - For deduplication, use composition `table + correlation_id + operation_id/transfer_id`, not timestamps
- **Presence guarantee**: If `correlation_id` is not provided by client, it **is not generated automatically** (remains `undefined`). Client must explicitly pass `correlation_id` in metadata for end-to-end tracing.
- **Propagation**: Passed to `router_policy_store` operations via `CorrelationId` parameter and included in telemetry metadata

**Recommendation**: Use **`x-correlation-id`** (lowercase) as the canonical metadata key for consistency with other services.

**Streaming RPC**: For streaming RPC (if added in the future), place `x-correlation-id` in **initial metadata** (request initial metadata). In `trailing metadata` (final metadata), place only service fields if necessary. This simplifies client behavior and ensures consistency with unary RPC.

**Bidi-streaming semantics**: For bidirectional streaming RPC:
- Client sends `x-correlation-id` in initial metadata once per session
- Server returns initial metadata once; then — message stream
- `x-correlation-id` applies to all messages within one session; in trailing metadata — only service fields (if necessary)

**Streaming retries**: When bidi-stream is interrupted:
- Preserve original `x-correlation-id` for the entire logical operation
- Generate new `operation_id` only when changing logical operation (not when restoring stream)
- For stream restoration, reuse identifiers of current operation (`x-correlation-id` + current `operation_id`)
- This ensures correct tracing and deduplication when restoring connection

**Usage example**:
```erlang
%% Client passes correlation_id in metadata
Ctx = #{metadata => [
    {<<"x-correlation-id">>, <<"550e8400-e29b-41d4-a716-446655440000">>}
]},

%% Server extracts and propagates
CorrId = router_admin_grpc:extract_correlation_id(Ctx),
{ok, Policies} = router_policy_store:list_policies(TenantId, CorrId).
```

### Retries and Idempotency

- Clients on automatic retries must preserve original `x-correlation-id` and, if present, `operation_id`/`transfer_id`.
- For idempotent RPC, this ensures correct tracing and consistent metrics.
- In client examples, indicate reuse of identifiers on retries.

### Idempotent RPC

**Checklist for clients**:

1. **Preserving identifiers on retries**:
   - On automatic retries, preserve original `x-correlation-id` from first request
   - If `operation_id`/`transfer_id` was passed in first request, preserve it for all retries
   - Do not generate new identifiers for retry attempts

2. **Idempotent operations**:
   - `UpsertPolicy`: idempotent (repeated call with same data gives same result)
   - `DeletePolicy`: idempotent (repeated deletion of non-existent policy returns `NOT_FOUND`, but is not an error)
   - `GetPolicy`: idempotent (read does not change state)
   - `ListPolicies`: idempotent (read does not change state)

3. **Tracing and metrics**:
   - Using one `x-correlation-id` for all retries ensures end-to-end operation tracing
   - Metrics are aggregated by `correlation_id`, providing correct performance picture accounting for retries

4. **Implementation example**:
   ```erlang
   %% Client preserves correlation_id for all retries
   CorrelationId = <<"550e8400-e29b-41d4-a716-446655440000">>,
   Metadata = [{<<"x-correlation-id">>, CorrelationId}],
   
   %% First attempt
   case call_rpc(Request, Metadata) of
       {ok, Response} -> Response;
       {error, retriable} ->
           %% Retry with same correlation_id
           call_rpc(Request, Metadata)  %% Use same Metadata
   end
   ```

### Retry Policy (Recommendations)

- Use exponential backoff with jitter (e.g., full jitter).
- Limit retries by budget: maximum number of attempts and/or maximum total duration.
- Preserve original `x-correlation-id` and `operation_id`/`transfer_id` for all attempts.
- Align retry budgets with alert thresholds (`latency_crit_ms`, `queue_crit`) to avoid "thundering herd" effect.
- For idempotent RPC, retries are allowed; for non-idempotent — avoid automatic repeats without confirmation.
- **gRPC Service Config**: When using gRPC Service Config for retry policies:
  - Limit retries for non-idempotent methods (including manual retry disabling)
  - Bind policy to specific methods, not entire service
  - Ensure metadata (`x-correlation-id`, `operation_id`) is preserved between attempts within retry policy
  - **Backoff parameters**: use exponential backoff with full jitter (not linear backoff)
    - **Minimum delay**: 100ms (base delay before first retry)
    - **Maximum delay**: 30s (maximum delay between attempts)
    - **Multiplier**: 2.0 (exponential growth: `delay = min(max_delay, base_delay * (multiplier ^ attempt))`)
    - **Full jitter**: `actual_delay = random(0, calculated_delay)` (uniform distribution from 0 to calculated delay)
    - **Example**: for attempt 3: `calculated_delay = min(30000, 100 * (2.0 ^ 3)) = 800ms`, `actual_delay = random(0, 800)ms`

### Client Interceptors and Retries

- Client interceptors/middleware must inject `x-correlation-id` and, if present, `operation_id`/`transfer_id` on each call attempt.
- In environments with retry policies (service config), ensure metadata is preserved between attempts.
- Verify retries in tests: all retries pass the same values of `x-correlation-id` and `operation_id`/`transfer_id`.

### Deadlines and RPC Cancellation

- On `deadline` expiration or client cancellation, `x-correlation-id` is preserved and applied to session events.
- It is recommended to publish `rpc_cancelled` event with reason (e.g., `deadline_exceeded`, `client_cancelled`) in telemetry.
- For alerts, use `service` + cancellation reason; avoid including `correlation_id` in metric labels.

### Event Publication Policy: timeout vs cancel

- **Timeout (`DEADLINE_EXCEEDED`)**: publish `rpc_timeout` with `wait_duration_us`, `queue_len`, `reason = deadline_exceeded`.
- **Client cancellation (`CANCELLED`)**: publish `rpc_cancelled` with `reason = client_cancelled`.
- Both branches use common metadata (`service`, `otp_version`, `correlation_id`) and do not add high-cardinality labels to metrics.

### Other Metadata

- `tenant_id`: tenant identifier; required in admin operations
- `policy_id`: policy identifier; required for `get`/`delete`
- `table`: ETS table name (`policy_store` or `policy_store_index`); used in transfer events and operations on specific ETS
- `result`: `ok | error` (standardized for all events)
- `error`: machine-readable error code (atom or binary, e.g.: `not_found`, `invalid_policy`, `timeout`); only if `result = error`

## Telemetry and Observability

### Events

#### Admin (`router_admin`)

**Module**: [`router_admin_grpc.erl`](../src/router_admin_grpc.erl)

- `[router_admin, upsert]` - create/update policy via Admin API
- `[router_admin, delete]` - delete policy via Admin API
- `[router_admin, list]` - get list of policies via Admin API
- `[router_admin, get]` - get a single policy via Admin API (note: telemetry is also generated in `router_policy_store` via `exec_with_telemetry`)

#### Router.Decide Events

**Module**: [`router_grpc.erl`](../src/router_grpc.erl)

**Note**: In the current implementation, Router.Decide (`router_grpc.erl`) does not publish telemetry events. If publication is added in the future, it is recommended to use the following pattern (semantics and metadata should repeat admin/store patterns for consistency):

- `[router_decide, decide]` - making routing decision
  - **Metadata**: `tenant_id`, `policy_id`, `provider_id` (selected provider), `result` (`ok`|`error`), `error` (if applicable), `correlation_id` (may be `undefined`)
  - **Measurements**: `duration_us`, `queue_len`
  - **Note**: When enabling event publication, use the same `metadata` and `measurements` fields as in admin/store events (including `correlation_id`, `duration_us`, `queue_len`, `result`)

#### Policy Store (`router_policy_store`)

**Module**: [`router_policy_store.erl`](../src/router_policy_store.erl) for policy operations, [`router_policy_store_heir.erl`](../src/router_policy_store_heir.erl) for transfer events

- `[router_policy_store, upsert]` - create/update policy
- `[router_policy_store, delete]` - delete policy
- `[router_policy_store, list]` - get list of policies
- `[router_policy_store, get_policy]` - get a single policy (via `get_policy/3`)
- `[router_policy_store, rebuild_index]` - rebuild index
- `[router_policy_store, transfer_attempt]` - attempt to transfer table (when recovering after process crash)
- `[router_policy_store, transfer_success]` - successful table transfer (includes `wait_duration_us` in measurements)
- `[router_policy_store, transfer_timeout]` - table transfer timeout (includes `wait_duration_us` in measurements)
- `[router_policy_store, transferred_to_heir]` - table transferred to heir when process crashes

### Metadata (Unified Format)

All events contain the following metadata:

- `tenant_id` - tenant identifier (required for all policy operations)
- `policy_id` - policy identifier (if applicable: for `upsert`, `delete`, `get_policy`)
- `correlation_id` - correlation identifier for end-to-end tracing (optional, from gRPC metadata via `x-correlation-id` or `correlation-id`)
  - **Consistency**: Field `correlation_id` is always present in metadata with value `undefined` if not provided by client (for event schema uniformity)
- `table` - table name (`policy_store` or `policy_store_index`); only for `router_policy_store` events (canonical field, used instead of `table_name`)
- `result` - operation result (`ok` or `error`) - standardized for all events
- `error` - machine-readable error code (only if `result = error`, e.g.: `invalid_policy`, `not_found`, `timeout`)
- `count` - number of entities in response (for `list` operations: number of policies in response)
- `otp_version` - OTP version (optional, recommended default for matrix deployments; source: `erlang:system_info(otp_release)` as string)
- `service` - service name (optional, recommended default for dashboard filtering; values: `router_admin`, `router_policy_store`, `router_decide`; added in each event emitter for consistency between services)

### Measurements

All events contain the following measurements:

- `duration_us` - operation duration in microseconds (µs) (uses `erlang:monotonic_time()` and `erlang:convert_time_unit/3`)
- `queue_len` - absolute value of gen_server mailbox queue length at operation completion (obtained via `process_info(self(), message_queue_len)`, dimensionless quantity, unitless counter)
- `count` - number of entities in response (for `list` operations: number of policies in response; for `upsert`/`delete`: always `1`)
- `wait_duration_us` - table owner wait time for transfer events in microseconds (µs) (only for `transfer_success` and `transfer_timeout`)

**Measurement units** (for reference):
- `duration_us`, `wait_duration_us` — microseconds (µs)
- `queue_len` — unitless counter (dimensionless quantity)
- `count` — number of entities (dimensionless quantity)

### Semantics of `count`

- **For `list`**: number of policies returned in the response (with pagination — number of elements on the current page)
- **For `upsert`/`delete`**: always `1` (one operation)
- **For `get_policy`**: absent (not published)

### Event Delivery and Deduplication

- Telemetry publication has "at-least-once" semantics.
- Consumers must perform deduplication by composition `table + correlation_id + operation_id/transfer_id` (if present).
- For aggregates (dashboards/alerts), use deduplication at the request or source level.
- **Consumer restart resilience**: Consumer-side deduplication must account for consumer restarts (resilience to re-reading partitions/checkpoints)
  - Use persistent storage for deduplication (e.g., Redis, database) with TTL window
  - When consumer restarts, deduplication should continue working based on saved keys within the TTL window
  - Account for clock skew when configuring TTL for cross-DC deployments

### Idempotency and Deduplication Window

- For telemetry consumers, we recommend a TTL deduplication window (e.g., 10–30 minutes) for the key `table + correlation_id + operation_id/transfer_id`.
- If `operation_id/transfer_id` is absent, use `table + correlation_id`; adjust the window depending on load and end-to-end latency.
- On the server, `operation_id/transfer_id` is generated if absent in the request and published in telemetry for tracing.
- Document the idempotency window for client commands to avoid double effects on retries.
- **Clock skew**: When deduplicating with TTL, account for time desynchronization between source and consumer, especially in cross-DC deployments
  - It is recommended to increase the TTL deduplication window by the maximum clock skew amount (e.g., +5–10 minutes)
  - Use composition `table + correlation_id + operation_id/transfer_id` instead of timestamps for event correlation

**Event examples**:
```erlang
%% Successful list
telemetry:execute([router_admin, list], #{count => 5}, #{
    tenant_id => <<"my_tenant">>,
    result => ok,
    correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>
}).

%% Successful get_policy
telemetry:execute([router_policy_store, get_policy], 
    #{duration_us => 1234, queue_len => 0}, 
    #{
        tenant_id => <<"my_tenant">>,
        policy_id => <<"my_policy">>,
        table => policy_store,  %% Exact ETS table name: policy_store
        result => ok,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>
    }).

%% Error get_policy (not_found)
telemetry:execute([router_policy_store, get_policy], 
    #{duration_us => 567, queue_len => 0}, 
    #{
        tenant_id => <<"my_tenant">>,
        policy_id => <<"nonexistent">>,
        table => policy_store,  %% Exact ETS table name: policy_store
        result => error,
        error => not_found,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>
    }).

%% Error on delete
telemetry:execute([router_admin, delete], #{count => 1}, #{
    tenant_id => <<"my_tenant">>,
    policy_id => <<"my_policy">>,
    result => error,
    error => not_found,
    correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>
}).
```

## Error Handling

### Unified Model

All operations use a unified error model:

- **Success**: `result => ok` in telemetry metadata
- **Error**: `result => error` in telemetry metadata, `error` field contains machine-readable code

### Error Structure

In telemetry metadata:
- `error` - machine-readable error code (atom, e.g.: `not_found`, `invalid_policy`, `timeout`, `ets_error`)
- **Consistency**: Field `error` is always present in metadata when `result => error`, even if value is `undefined` (for schema uniformity)

In gRPC response:
- gRPC Status Code (e.g., `INVALID_ARGUMENT`, `NOT_FOUND`, `INTERNAL`)
- Error message (human-readable description)
- Structured details via `grpc-status-details-bin` (for `INTERNAL`, `UNAVAILABLE`, `RESOURCE_EXHAUSTED`, `DEADLINE_EXCEEDED`)

**Mapping of atoms to gRPC Status Code** (for convenient mapping):

| Atom (telemetry) | gRPC Status Code | Numeric Code |
|------------------|------------------|--------------|
| `not_found` (canonical), `policy_not_found` (alias for Router.Decide) | `NOT_FOUND` | 5 |
| `invalid_policy`, `invalid_argument`, `invalid_request` | `INVALID_ARGUMENT` | 3 |
| `timeout`, `gen_server_timeout` | `DEADLINE_EXCEEDED` | 4 |
| `ets_error`, `internal_error` | `INTERNAL` | 13 |

**Note for alerts**: Use the canonical atom `not_found` in dashboards and alerts; consider `policy_not_found` only as an alias for Router.Decide. This ensures monitoring consistency and reduces discrepancies.

**Location of reasons**:
- **For RPC-level errors**: human-readable `message` in gRPC Status (e.g., "queue wait exceeded 50000µs", "Upstream service temporarily unavailable")
- **For telemetry**: compact `reason` atom in metadata (e.g., `deadline_exceeded`, `upstream_unreachable`, `queue_overflow`)
- Mapping `reason` → `message` is provided in table above and in RPC response examples

### Canonical reasons (summary)

- **`unavailable`**: `upstream_unreachable`, `transport_error`, `resolver_failure`
- **`resource_exhausted`**: `queue_overflow`, `concurrency_limit`, `rate_limited`
- **`deadline_exceeded`**: `queue_wait_timeout`, `processing_timeout`
- **`cancelled`**: `client_cancelled` (initiated by client), `deadline_exceeded` (client deadline)

### List of Error Atoms

**Standardized error codes** (for monitoring and aggregation):

| Atom | Description | When Used | gRPC Status |
|------|----------|---------------------|-------------|
| `not_found` | Resource not found (canonical atom) | Policy does not exist for `get`/`delete` | `NOT_FOUND` (5) |
| `policy_not_found` | Policy not found (alias for Router.Decide) | Policy does not exist (Router.Decide) | `NOT_FOUND` (5) |
| `invalid_policy` | Invalid policy | Invalid weights, empty `policy_id`, duplicate provider IDs | `INVALID_ARGUMENT` (3) |
| `invalid_argument` | Invalid argument | Invalid request (missing required field) | `INVALID_ARGUMENT` (3) |
| `invalid_request` | Invalid request | Invalid Protobuf or request structure | `INVALID_ARGUMENT` (3) |
| `timeout` | Operation timeout | Wait time exceeded (transfer, gen_server) | `DEADLINE_EXCEEDED` (4) |
| `gen_server_timeout` | gen_server timeout | Response wait time from gen_server exceeded | `DEADLINE_EXCEEDED` (4) |
| `ets_error` | ETS error | Error when working with ETS table | `INTERNAL` (13) |
| `internal_error` | Internal error | Unexpected processing error | `INTERNAL` (13) |

**Recommended**: Use this stable set of codes for monitoring, alerts, and error aggregation.

**Examples of RPC responses with gRPC Status** (to understand the actual response schema):

**INVALID_ARGUMENT** (3) — invalid policy:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_INVALID_ARGUMENT, <<"Invalid policy: weights must sum to 100">>}}

%% Telemetry metadata
#{
    result => error,
    error => invalid_policy
}
```

**NOT_FOUND** (5) — policy not found:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_NOT_FOUND, <<"Policy not found: policy_id=non-existent">>}}

%% Telemetry metadata
#{
    result => error,
    error => not_found
}
```

**INTERNAL** (13) — internal ETS error:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_INTERNAL, <<"Internal error: ETS table access failed">>}}

%% Telemetry metadata
#{
    result => error,
    error => ets_error
}
```

**DEADLINE_EXCEEDED** (4) — operation timeout:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_DEADLINE_EXCEEDED, <<"queue wait exceeded 50000µs">>}}

%% Telemetry metadata
#{
    result => error,
    error => deadline_exceeded,
    wait_duration_us => 50000,
    queue_len => 150
}

%% rpc_timeout event (if published)
telemetry:execute([router_admin, rpc_timeout], 
    #{wait_duration_us => 50000, queue_len => 150}, 
    #{
        service => router_admin,
        otp_version => <<"26">>,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>,
        reason => deadline_exceeded
    }).
```

**Alert recommendations for DEADLINE_EXCEEDED**: Thresholds `latency_crit_ms`, `queue_crit` are bound to `service`; deduplication by `table + correlation_id + operation_id/transfer_id`.

**CANCELLED** (1) — cancelled by client:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_CANCELLED, <<"client cancelled the request">>}}

%% Telemetry metadata
#{
    result => error,
    error => cancelled
}

%% rpc_cancelled event (if published)
telemetry:execute([router_admin, rpc_cancelled], 
    #{duration_us => 25000, queue_len => 5}, 
    #{
        service => router_admin,
        otp_version => <<"26">>,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>,
        reason => client_cancelled
    }).
```

**Alert recommendations for CANCELLED**:
- Filter by `service`; avoid including `correlation_id` in metrics.
- Monitor cancellation spikes (often coincides with network failures/exceeding client deadlines).

**UNAVAILABLE** (14) — provider/backend temporarily unavailable:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_UNAVAILABLE, <<"Upstream service temporarily unavailable">>}}

%% Telemetry metadata
#{
    result => error,
    error => unavailable
}

%% rpc_unavailable event (if published)
telemetry:execute([router_admin, rpc_unavailable], 
    #{duration_us => 120000, queue_len => 10}, 
    #{
        service => router_admin,
        otp_version => <<"26">>,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>,
        reason => upstream_unreachable
    }).
```

**Alert recommendations for UNAVAILABLE**: Distinguish network failures from queues/timeouts; filter by `service` + `reason`; avoid including `correlation_id` in metrics.

**RESOURCE_EXHAUSTED** (8) — queue overflow or worker limits:
```erlang
%% gRPC Status
{grpc_error, {?GRPC_STATUS_RESOURCE_EXHAUSTED, <<"Queue overflow: queue_len exceeded limit">>}}

%% Telemetry metadata
#{
    result => error,
    error => resource_exhausted,
    queue_len => 1500
}

%% rpc_resource_exhausted event (if published)
telemetry:execute([router_admin, rpc_resource_exhausted], 
    #{queue_len => 1500, duration_us => 5000}, 
    #{
        service => router_admin,
        otp_version => <<"26">>,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>,
        reason => queue_overflow
    }).
```

**Alert recommendations for RESOURCE_EXHAUSTED**: Thresholds `queue_crit` are bound to `service`; deduplication by `table + correlation_id + operation_id/transfer_id`.

### Retryability (UNAVAILABLE vs RESOURCE_EXHAUSTED)

- **`UNAVAILABLE (14)`**: considered a temporary error; retries are allowed with exponential backoff (full jitter) and limited budget.
- **`RESOURCE_EXHAUSTED (8)`**: retries are allowed only when recovery signs are present (e.g., `queue_len` decrease); otherwise — escalation/alert.
- Always preserve original `x-correlation-id` and `operation_id/transfer_id` for correct tracing and deduplication.

**Examples**:
```erlang
%% Error when deleting non-existent policy
telemetry:execute([router_admin, delete], #{count => 1}, #{
    tenant_id => <<"t-123">>,
    policy_id => <<"p-404">>,
    correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>,
    result => error,
    error => not_found
}).

%% gRPC response
{grpc_error, {?GRPC_STATUS_NOT_FOUND, <<"policy not found">>}}
```

## Performance

### Operation Complexity

#### ListPolicies

**Complexity**: `O(k) + O(k log k)` where `k` = number of policies for a single tenant

- **Selection**: `O(k)` - using secondary index (bag) for O(1) lookup by `tenant_id`
- **Sorting**: `O(k log k)` - deterministic sorting by `policy_id` using `binary:compare/2`

**Note**: `k` is usually significantly smaller than `n` (number of policies for one tenant << total number of policies)

**Deterministic sorting**:
- Results are sorted by `policy_id` in lexicographic order
- Uses `binary:compare/2` for consistent order between calls
- Clients can rely on stable result order

#### Other Operations

- **upsert/delete/get**: amortized `O(1)` for ETS (depending on table type and load)

### Threshold Values

Performance and queue thresholds are configured via `application:get_env(beamline_router, ...)` and loaded once into `State` with unit unification (microseconds for durations).

**Configuration examples** (keys `application:get_env(beamline_router, ...)`):
- `latency_warn_ms` (default: 10ms) - warning when exceeded
- `latency_crit_ms` (default: 50ms) - critical value
- `list_policies_latency_warn_ms` (default: 5ms) - warning for list operations
- `list_policies_latency_crit_ms` (default: 50ms) - critical value for list operations
- `queue_warn` (default: 100) - warning when queue length exceeded
- `queue_crit` (default: 1000) - critical queue length value
- `policy_count_warn` (default: 1000) - warning when large number of policies

### Monitoring Recommendations

- **Alert** when `duration_us` > critical threshold (from `application:get_env(beamline_router, latency_crit_ms)` or `list_policies_latency_crit_ms` for list operations)
- **Track** `queue_len` to detect backpressure and concurrency issues (threshold: `queue_warn` / `queue_crit`)
- **For `list`**: monitor `count` for spikes (anomalously large responses, threshold: `policy_count_warn`)
- **For transfer**: monitor `wait_duration_us`; timeouts — separate alert

### Dashboards and Alerts

**Recommended dashboard widgets**:

- **Percentiles of `duration_us`**: P50, P95, P99 by operations (`upsert`, `delete`, `list`, `get_policy`)
- **`queue_len`**: Average and maximum by nodes/processes
- **Frequency of `transfer_*`**: Number of `transfer_attempt`, `transfer_success`, `transfer_timeout` events per period
- **`wait_duration_us`**: Average and maximum for transfer events
- **Distribution of `count`**: Histogram of `count` for `list` operations (detecting anomalously large responses)

**Recommendation for matrix deployments** (recommended default): Add `otp_version` and `service` tags to event metadata for graph filtering:
- `otp_version`: OTP version (e.g., `otp_version => <<"26">>`)
- `service`: service name (`router_admin`, `router_policy_store`, `router_decide`) — simplifies dashboard construction

**Dashboard filters**: Fix unified `service` values (`router_admin`, `router_policy_store`, `router_decide`) and include them in alerts. This simplifies analysis in matrix deployments by `otp_version`.

**Recommended alerts** (connection with thresholds from `application:get_env(beamline_router, ...)`):

**Alert signature**: All alerts must use consistent `service` values (`router_admin`, `router_policy_store`, `router_decide`) as primary filters and include them in the alert signature (in all rules). This ensures filter consistency and prevents drift on releases.

### Metric Cardinality

- **Avoid using `correlation_id` as a metric label** due to high cardinality (each request has a unique identifier).
- **Allowed labels**: `service`, `otp_version`, event/operation type (e.g., `upsert`, `delete`, `list`).
- **`correlation_id` and `operation_id`/`transfer_id`** keep in logs/traces; do not include in metric labels.
- This prevents metric cardinality explosion and ensures monitoring system stability.

### Playbook: Diagnostics and Correlation

- **Check `queue_len` and `wait_duration_us`** by `service`; compare with thresholds `latency_crit_ms`, `queue_crit`.
- **Enable event deduplication** by `table + correlation_id + operation_id/transfer_id`.
- **Verify client retries** (backoff, full jitter) and retry budgets; avoid linear backoff.
- **Account for `clock skew`**: in cross-DC, increase deduplication TTL by 5–10 minutes.

### Playbook: Correlation with System Events

- **`unavailable`**: correlate spikes with network/balancer/resolver logs, check upstream health.
- **`resource_exhausted`**: check worker limits, `queue_len`, incoming traffic changes (deployment/campaigns).
- **`deadline_exceeded`**: correlate `wait_duration_us` with thresholds `latency_crit_ms` and client retry policy.

- **`duration_us` > critical threshold**: Alert when exceeding `latency_crit_ms` (for general operations) or `list_policies_latency_crit_ms` (for list operations)
  - **Filters**: `service` (required), `otp_version` (optional)
- **`transfer_timeout` frequency/ratio > threshold**: Alert when exceeding acceptable number of transfer timeouts (e.g., > 10% of all transfer operations)
  - **Filters**: `service = router_policy_store` (required), `otp_version` (optional)
- **`queue_len` > `queue_crit`**: Alert when exceeding critical queue length (key: `queue_crit`, default: 1000)
  - **Filters**: `service` (required), `otp_version` (optional)
- **`count` for `list` > `policy_count_warn`**: Warning when large number of policies in response (key: `policy_count_warn`, default: 1000)
  - **Filters**: `service` (required), `otp_version` (optional)

**Mini examples of queries/alerts**:

**Timeout alert**:
- Filter: `service` (required), `error = deadline_exceeded` (optional)
- Thresholds: `latency_crit_ms` and `queue_crit` from `application:get_env(beamline_router, ...)`
- Example query: `service = router_admin AND error = deadline_exceeded AND wait_duration_us > 50000`
- Deduplication: by `table + correlation_id + operation_id/transfer_id`

**Cancellation spike alert**:
- Filter: `service` (required), `error = cancelled` (optional)
- Correlation check: with client deadlines/network incidents
- Example query: `service = router_admin AND error = cancelled AND count > 100 per 5min`
- Avoid including `correlation_id` in metric labels

**See also**: [PERFORMANCE.md](PERFORMANCE.md) for performance details and threshold values.

## ETS Transfer Protocol

### Recovery Stages

1. **Claim**: New process calls `router_policy_store_heir:claim(self())`
2. **Wait with Retry**: Wait for ETS owner with two-phase retry:
   - First wait: `TRANSFER_TIMEOUT_MS` (default: 1000ms)
   - If timeout: short retry `TRANSFER_RETRY_MS` (default: 500ms) before fallback
3. **Fallback Create**: If transfer fails, a new table is created

### Transfer Events

- `[router_policy_store, transfer_attempt]` - attempt to transfer table (when recovering after process crash)
- `[router_policy_store, transfer_success]` - successful table transfer (includes `wait_duration_us` in measurements)
- `[router_policy_store, transfer_timeout]` - table transfer timeout (includes `wait_duration_us` in measurements)
- `[router_policy_store, transferred_to_heir]` - table transferred to heir when process crashes

### Requirements

- **Transfer of both tables**: Main table (`policy_store`) and index table (`policy_store_index`) must be transferred together (exact ETS table names: `policy_store` and `policy_store_index`)
- **Paired events**: For each table (main and index), separate `transfer_attempt`, `transfer_success`/`transfer_timeout` events are generated
- **Write Access Verification**: After successful `claim` of both tables, write to `protected` table is verified to confirm access rights
- **Tests**: Cover write to `protected` table after `claim` and correctness of rights

**Transfer event examples**:

```erlang
%% transferred_to_heir (when process crashes)
telemetry:execute([router_policy_store, transferred_to_heir], 
    #{}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store (canonical field)
        from => <0.123.0>  %% Pid of crashed process
    }).

%% transfer_success for main table (policy_store)
telemetry:execute([router_policy_store, transfer_success], 
    #{wait_duration_us => 51234}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store
        result => ok
    }).

%% transfer_success for index table (policy_store_index) - paired event
telemetry:execute([router_policy_store, transfer_success], 
    #{wait_duration_us => 52345}, 
    #{
        table => policy_store_index,  %% Exact ETS table name: policy_store_index
        result => ok
    }).

%% transfer_timeout (if transfer failed)
telemetry:execute([router_policy_store, transfer_timeout], 
    #{wait_duration_us => 100000}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store
        result => error
    }).
```

**Example of paired transfer events** (for one recovery operation):

```erlang
%% When recovering after process crash, paired events are generated for both tables:

%% 1. transfer_attempt for main table (policy_store)
telemetry:execute([router_policy_store, transfer_attempt], 
    #{}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store
        result => ok
    }).

%% 2. transfer_attempt for index table (policy_store_index)
telemetry:execute([router_policy_store, transfer_attempt], 
    #{}, 
    #{
        table => policy_store_index,  %% Exact ETS table name: policy_store_index
        result => ok
    }).

%% 3. transfer_success for main table (policy_store)
telemetry:execute([router_policy_store, transfer_success], 
    #{wait_duration_us => 51234, duration_us => 72000, queue_len => 0}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store
        result => ok
    }).

%% 4. transfer_success for index table (policy_store_index) - paired event
telemetry:execute([router_policy_store, transfer_success], 
    #{wait_duration_us => 52345, duration_us => 73000, queue_len => 0}, 
    #{
        table => policy_store_index,  %% Exact ETS table name: policy_store_index
        result => ok
    }).
```

**Note**: When recovering after process crash, paired events are generated for main (`policy_store`) and index (`policy_store_index`) tables. For one recovery operation, there can be two `transfer_attempt` events (one for each table), followed by two `transfer_success` or `transfer_timeout` events.

**Event sequence for each table**: For each table (main and index), the following event sequence is possible:
- `transfer_attempt` → `transfer_success` (successful transfer)
- `transfer_attempt` → `transferred_to_heir` (transfer on process crash)
- `transfer_attempt` → `transfer_timeout` (transfer timeout)

**Important for observability**: Events for main and index tables may be published with slight time desynchronization (a few microseconds), as table transfers are performed sequentially. 

**To correlate paired events, use**:
- **`table`** in metadata to distinguish main (`policy_store`) and index (`policy_store_index`) tables
- **`correlation_id`** (if provided by client) to group events of one recovery operation
- **`operation_id`** or **`transfer_id`** (if present in events) for additional correlation of paired events
- **Should not** strictly synchronize paired events by time due to microsynchronization

**Correlation of paired events**: Correlation of event pairs (main and index tables) is performed by combination `table` + `correlation_id`; if `operation_id` or `transfer_id` field is present in events — additionally by this field for more accurate grouping.

**Format of `operation_id`/`transfer_id`**: Recommended format — ULID with monotonic generation or UUID v4. If absent in request — generation on server side and publication in telemetry to ensure end-to-end tracing.

**See also**: [ETS_SEMANTICS.md](ETS_SEMANTICS.md) for transfer protocol details and fault tolerance.

## Security

### Prohibition of Secret Logging

**Critical requirement**: Secrets (API keys, tokens, passwords) **must never** appear in logs.

**Masking practices**:
- All errors go through `sanitize_error_value/1` before logging
- Secrets are replaced with `[REDACTED: contains sensitive data]`
- Verification is performed in E2E tests: `router_secrets_logging_SUITE`

**E2E test**: `test_api_key_not_logged/1` in `router_secrets_logging_SUITE.erl` verifies that API keys do not appear in error messages.

**Masking example**:
```erlang
%% Error with secret
Error = {error, <<"Connection failed with api_key=secret123">>},

%% After masking
Sanitized = router_admin_grpc:sanitize_error_value(Error),
%% Sanitized = {error, <<"[REDACTED: contains sensitive data]">>}
```

### Metadata Security

- **Length limitation**: it is recommended to limit `x-correlation-id` and `operation_id/transfer_id` (e.g., ≤128 characters).
- **Maximum metadata size**: maximum allowed metadata size per header — ≤8KB (including all keys and values).
- **Truncation policy**: values exceeding limits must be truncated or normalized; on violations, publish `invalid_correlation_id` with `reason` (e.g., `too_long`, `exceeds_limit`).
- All metadata values are untrusted; perform normalization and format validation before use.
- **Logging**: avoid injections and PII; on violations, publish `invalid_correlation_id` with `reason`, apply rate-limit/sampling.

### Authorization

All RouterAdmin methods require authorization via gRPC request metadata:
- `x-api-key` header or `authorization` header (Bearer token)
- API key is configured via `application:get_env(beamline_router, admin_api_key)`

**Authorization errors**:
- `UNAUTHENTICATED` (16): missing or invalid API key
- Secrets are not logged even on authorization errors

## Usage Examples

### Calling ListPolicies with Metadata

**Proto**: [`apps/otp/router/proto/beamline/flow/v1/flow.proto`](../proto/beamline/flow/v1/flow.proto) — service `RouterAdmin`, RPC `ListPolicies`, message `ListPoliciesRequest`

**Client** adds `correlation_id` to gRPC metadata (canonical key `x-correlation-id` is used):

**Example for grpc-go**:
```go
import (
    "context"
    "google.golang.org/grpc/metadata"
)

ctx := context.Background()
correlationID := "550e8400-e29b-41d4-a716-446655440000"
md := metadata.New(map[string]string{
    "x-correlation-id": correlationID,  // Canonical key (lowercase)
})
ctx = metadata.NewOutgoingContext(ctx, md)

// Call gRPC method
response, err := client.ListPolicies(ctx, &pb.ListPoliciesRequest{
    TenantId: "t-123",
})
```

**Example for Python (grpc-python)**:
```python
import grpc
from beamline.flow.v1 import flow_pb2, flow_pb2_grpc

correlation_id = "550e8400-e29b-41d4-a716-446655440000"
metadata = [('x-correlation-id', correlation_id)]  # Canonical key (lowercase)

channel = grpc.insecure_channel('localhost:9000')
stub = flow_pb2_grpc.RouterAdminStub(channel)

request = flow_pb2.ListPoliciesRequest(tenant_id="t-123")
response = stub.ListPolicies(request, metadata=metadata)
```

**Example for Node.js (grpc-node/@grpc/grpc-js)**:
```javascript
const grpc = require('@grpc/grpc-js');
const { RouterAdminClient } = require('./generated/flow_grpc_pb');
const { ListPoliciesRequest } = require('./generated/flow_pb');

const correlationId = '550e8400-e29b-41d4-a716-446655440000';
const metadata = new grpc.Metadata();
metadata.add('x-correlation-id', correlationId);  // Canonical key (lowercase)

const client = new RouterAdminClient('localhost:9000', grpc.credentials.createInsecure());
const request = new ListPoliciesRequest();
request.setTenantId('t-123');

client.listPolicies(request, metadata, (error, response) => {
    if (error) {
        console.error('Error:', error);
    } else {
        console.log('Response:', response);
    }
});
```

**Example for Erlang**:
```erlang
Ctx = #{metadata => [
    {<<"x-correlation-id">>, <<"550e8400-e29b-41d4-a716-446655440000">>}  %% Canonical key (lowercase)
]},
Request = flow_pb:encode_msg(#'ListPoliciesRequest'{tenant_id = <<"t-123">>}, 'ListPoliciesRequest'),
{ok, Response, _} = router_admin_grpc:list_policies(Ctx, Request).
```

**Server** extracts `correlation_id` and passes it to `Meta` and telemetry:
```erlang
CorrId = router_admin_grpc:extract_correlation_id(Ctx),
{ok, Policies} = router_policy_store:list_policies(TenantId, CorrId),
%% correlation_id is automatically included in telemetry metadata
```

### Example transfer_success Event

```erlang
telemetry:execute([router_policy_store, transfer_success], 
    #{wait_duration_us => 51234, duration_us => 72000, queue_len => 0}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store
        result => ok
    }).
```

### Example transferred_to_heir Event

```erlang
%% Event is generated when router_policy_store process crashes
%% Table is automatically transferred to heir
telemetry:execute([router_policy_store, transferred_to_heir], 
    #{}, 
    #{
        table => policy_store,  %% Exact ETS table name: policy_store (canonical field)
        from => <0.123.0>  %% Pid of crashed process
    }).
```

## Pagination (Recommendations)

For `ListPolicies`, it is recommended to add pagination to proto:

- **Request**: `page_size`, `page_token` (or `offset`/`limit`)
- **Response**: `next_page_token` in response

**In telemetry**:
- `count` remains "number of policies in response on current page"
- Additional metadata: `page_size` if necessary

**Note**: Pagination is already defined in `flow.proto` (`ListPoliciesRequest` and `ListPoliciesResponse`), but implementation is deferred to the future.

## Compatibility and Versions

### CI/CD

- **CI publishes coverage** with artifact name: `router-coverage-${OTP_VERSION}-${BRANCH}-${SHA}`
- **Supported OTP versions**: `24`, `25`, `26` (matrix in CI)
- **Caching**: `rebar3` dependencies and Dialyzer PLT are cached to speed up builds

### Versioning

- **Important**: Update documentation when changing proto/metadata/events
- **Protobuf**: Canonical path `apps/otp/router/proto/beamline/flow/v1/flow.proto`
- **Generation**: Uses `rebar3 gpb compile` to generate Erlang code from proto

## See Also

### Documentation

- `apps/otp/router/proto/beamline/flow/v1/flow.proto` - Protobuf definition (source of truth)
- `apps/otp/router/docs/PERFORMANCE.md` - Performance details, thresholds, monitoring
- `apps/otp/router/docs/ETS_SEMANTICS.md` - ETS semantics, fault tolerance, transfer protocol
- `apps/otp/router/docs/GRPC_ERROR_CODES.md` - Error codes reference
- `apps/otp/router/docs/GENERATION.md` - Protobuf code generation

### Source Code

- `apps/otp/router/src/router_grpc.erl` - Router.Decide implementation
- `apps/otp/router/src/router_admin_grpc.erl` - RouterAdmin implementation
- `apps/otp/router/src/router_core.erl` - Routing core
- `apps/otp/router/src/router_decider.erl` - Decision algorithm
- `apps/otp/router/src/router_policy_store.erl` - Policy store (ETS)

### Tests

- `apps/otp/router/test/router_grpc_SUITE.erl` - Unit tests for Router.Decide
- `apps/otp/router/test/router_admin_grpc_integration_SUITE.erl` - Integration tests for RouterAdmin (8 tests: upsert, get, list, delete, checkpoint status, error handling)
- `apps/otp/router/test/router_admin_grpc_concurrency_SUITE.erl` - Concurrency tests for RouterAdmin (4 tests: concurrent upserts, race conditions, concurrent get/list/delete)
- `apps/otp/router/test/router_policy_store_SUITE.erl` - Unit tests for policy store
- `apps/otp/router/test/router_secrets_logging_SUITE.erl` - E2E tests for secret masking

## Conventions

This section describes unified conventions for creating new telemetry events and maintaining API consistency.

### Checklist for New Events

When adding a new telemetry event, ensure that:

1. **Event name**: Follows pattern `[prefix, operation]` (e.g., `[router_admin, upsert]`, `[router_policy_store, list]`)
2. **`result`**: Always present in metadata with value `ok` or `error`
3. **`correlation_id`**: Always present in metadata (may be `undefined` if not provided by client)
4. **`duration_us`**: Always present in measurements (operation duration in microseconds)
5. **`queue_len`**: Always present in measurements (gen_server mailbox size)
6. **`count`**: Published only for operations returning multiple results:
   - For `list`: number of elements in response (with pagination — on current page)
   - For `upsert`/`delete`: always `1` (one operation)
   - For `get_policy`: absent (not published)
7. **`error`**: Published only when `result => error` (error atom, e.g.: `not_found`, `invalid_policy`)
8. **`tenant_id`**: Published for all policy operations (required)
9. **`policy_id`**: Published for operations on specific policy (`upsert`, `delete`, `get_policy`)
10. **Idempotency and retries**: For idempotent RPC, ensure clients preserve original `x-correlation-id` and `operation_id`/`transfer_id` on retries for correct tracing and consistent metrics

### Event Template Example

```erlang
%% Successful operation
telemetry:execute([router_admin, operation], 
    #{duration_us => 1234, queue_len => 0, count => 1},  %% measurements
    #{
        tenant_id => <<"my_tenant">>,
        policy_id => <<"my_policy">>,  %% if applicable
        result => ok,
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>  %% may be undefined
    }).

%% Operation error
telemetry:execute([router_admin, operation], 
    #{duration_us => 567, queue_len => 0},  %% count absent for errors
    #{
        tenant_id => <<"my_tenant">>,
        policy_id => <<"my_policy">>,  %% if applicable
        result => error,
        error => not_found,  %% error atom
        correlation_id => <<"550e8400-e29b-41d4-a716-446655440000">>  %% may be undefined
    }).
```

### Implementation of `otp_version` and `service` in Telemetry

If you decide to include `otp_version` and `service` in telemetry events, follow these recommendations:

**Source of `otp_version`**:
- Use `erlang:system_info(otp_release)` as a string (e.g., `<<"26">>`)
- Get the value once at initialization and cache it in process state to avoid repeated calls

**Field `service`** (recommended default):
- Fix values at the level of each event emitter:
  - `router_admin` — for events from `router_admin_grpc.erl`
  - `router_policy_store` — for events from `router_policy_store.erl`
  - `router_decide` — for events from `router_grpc.erl` (Router.Decide)
- Value must be a constant for each module
- **Important**: Values are added in each event emitter to avoid discrepancies between services. This simplifies analysis in matrix deployments by `otp_version` and ensures unified dashboard filters.

**Addition logic**:
- Add `otp_version` and `service` uniformly in a common event publication wrapper (where `metadata`/`measurements` are formed) to avoid duplicating duplicates
- Example wrapper structure:
```erlang
%% In common event publication function
publish_telemetry(Event, Measurements, Metadata) ->
    %% Add otp_version and service uniformly
    EnrichedMetadata = Metadata#{
        otp_version => get_otp_version(),  %% Cached value
        service => get_service_name()       %% Constant for module
    },
    telemetry:execute(Event, Measurements, EnrichedMetadata).
```

**Example of minimal semantics** (without code changes, just specification):

- **Metadata**: `tenant_id`, `policy_id` (when applicable), `correlation_id`, `table` (transfer), `result`, `error` (atom), `otp_version`, `service`
- **Measurements**: `duration_us`, `queue_len`, `count` (semantics as in doc), `wait_duration_us` (transfer)

### Document Version

**Doc version**: Updated 2025-11-09 — added sections on retries, error handling, deduplication, and correlation with system events. When changing API/events/metadata, update this section with commit SHA or version.

**CI recommendation**: In CI, you can automatically substitute the actual SHA via environment variable (e.g., `GITHUB_SHA` in GitHub Actions) to link the document to a specific build. Example:
```yaml
# In CI workflow
- name: Update doc version
  run: |
    sed -i "s/Doc version:.*/Doc version: ${{ github.sha }}/" apps/otp/router/docs/GRPC_API.md
```

**Recommendation**: Link documentation to specific code state via commit SHA in comments or through documentation versioning system.
